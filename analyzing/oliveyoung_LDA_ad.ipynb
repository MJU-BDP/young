{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "advertisement = pd.read_csv('oliveyoung_advertisement_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>name</th>\n",
       "      <th>len_img</th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[2023어워즈/3년연속] 아누아 어성초 77 수딩 토너 350ml 어워즈 한정 기...</td>\n",
       "      <td>9</td>\n",
       "      <td>단독 어워즈 한정 기획 버워스 토너 부문 단독 어워즈 한정 기획 아누 어성초 수딩 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[12/4 하루특가][2023어워즈] 아비브 어성초 카밍 토너 스킨부스터 더블 기획...</td>\n",
       "      <td>4</td>\n",
       "      <td>올리브영 어워즈 진정 케어 기준 아비 어성초 토너 스킨 토너 어성초 토너 더블 어워...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[한정기획] 라네즈 크림스킨 170ml 리필 기획 (+170ml 리필)</td>\n",
       "      <td>10</td>\n",
       "      <td>크림 스킨 리필팩 기획 세트 기획 세트 기획 세트 구성 크림 스킨 리필팩 오특 비아...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[2023어워즈] 라운드랩 1025 독도 토너 500ml 어워즈 한정기획(+크림50...</td>\n",
       "      <td>17</td>\n",
       "      <td>단독 구성 자작나무 수분 크림 수분 진정 비타 일루 동시 차오 수분 필요 지치다 피...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[백현팬싸응모]브링그린 티트리시카 수딩 토너 500ml 어워즈 한정기획(+크림20m...</td>\n",
       "      <td>11</td>\n",
       "      <td>강력 트리 싸르 케어 올리브영 대표 트러블 토너 수상 올리브영 어워즈 스킨 토너 부...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>194</td>\n",
       "      <td>290</td>\n",
       "      <td>더마펌 울트라 수딩 토너 R4 200ml (수분진정)</td>\n",
       "      <td>9</td>\n",
       "      <td>피부 진정 순도 아줄렌 놀랍다 진정 능력 라인 민감 피부 찾다 저자극 진정 토너 더...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>291</td>\n",
       "      <td>아비브 어성초 카밍 토너 스킨부스터 500ml+화장솜 30매 증정 기획</td>\n",
       "      <td>3</td>\n",
       "      <td>단독 기획 어성초 토너 대용량 기획 진정 토너 저자극 진정 진정 저자극 피부결 케어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>292</td>\n",
       "      <td>헤이네이처 어성초 스킨 토너 모이스처라이저 150ml (촉촉)</td>\n",
       "      <td>6</td>\n",
       "      <td>어성초 스킨 타입 안내 올리브영 성조 스킨 촉촉 타입 운영 죽다 이다 구매 참고 바...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>293</td>\n",
       "      <td>더말로지카 멀티-액티브 토너 50ml</td>\n",
       "      <td>2</td>\n",
       "      <td>피부 전문가 의하다 만들다 어진 피부 전문 코스 브랜드 말로 서로 피부 밉다 해결 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>294</td>\n",
       "      <td>[박보검 PICK] 비오템 옴므 포스 수프림 토너 200ml</td>\n",
       "      <td>1</td>\n",
       "      <td>상품 설명 프록실린 성분 삼나무 추출 성분 즉각 피부 진정 피부 탄력 증가 토너 사...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  Unnamed: 0.1  \\\n",
       "0             0             0   \n",
       "1             1             1   \n",
       "2             2             2   \n",
       "3             3             3   \n",
       "4             4             4   \n",
       "..          ...           ...   \n",
       "194         194           290   \n",
       "195         195           291   \n",
       "196         196           292   \n",
       "197         197           293   \n",
       "198         198           294   \n",
       "\n",
       "                                                  name  len_img  \\\n",
       "0    [2023어워즈/3년연속] 아누아 어성초 77 수딩 토너 350ml 어워즈 한정 기...        9   \n",
       "1    [12/4 하루특가][2023어워즈] 아비브 어성초 카밍 토너 스킨부스터 더블 기획...        4   \n",
       "2              [한정기획] 라네즈 크림스킨 170ml 리필 기획 (+170ml 리필)       10   \n",
       "3    [2023어워즈] 라운드랩 1025 독도 토너 500ml 어워즈 한정기획(+크림50...       17   \n",
       "4    [백현팬싸응모]브링그린 티트리시카 수딩 토너 500ml 어워즈 한정기획(+크림20m...       11   \n",
       "..                                                 ...      ...   \n",
       "194                      더마펌 울트라 수딩 토너 R4 200ml (수분진정)        9   \n",
       "195            아비브 어성초 카밍 토너 스킨부스터 500ml+화장솜 30매 증정 기획        3   \n",
       "196                 헤이네이처 어성초 스킨 토너 모이스처라이저 150ml (촉촉)        6   \n",
       "197                               더말로지카 멀티-액티브 토너 50ml        2   \n",
       "198                  [박보검 PICK] 비오템 옴므 포스 수프림 토너 200ml        1   \n",
       "\n",
       "                                                    ad  \n",
       "0    단독 어워즈 한정 기획 버워스 토너 부문 단독 어워즈 한정 기획 아누 어성초 수딩 ...  \n",
       "1    올리브영 어워즈 진정 케어 기준 아비 어성초 토너 스킨 토너 어성초 토너 더블 어워...  \n",
       "2    크림 스킨 리필팩 기획 세트 기획 세트 기획 세트 구성 크림 스킨 리필팩 오특 비아...  \n",
       "3    단독 구성 자작나무 수분 크림 수분 진정 비타 일루 동시 차오 수분 필요 지치다 피...  \n",
       "4    강력 트리 싸르 케어 올리브영 대표 트러블 토너 수상 올리브영 어워즈 스킨 토너 부...  \n",
       "..                                                 ...  \n",
       "194  피부 진정 순도 아줄렌 놀랍다 진정 능력 라인 민감 피부 찾다 저자극 진정 토너 더...  \n",
       "195  단독 기획 어성초 토너 대용량 기획 진정 토너 저자극 진정 진정 저자극 피부결 케어...  \n",
       "196  어성초 스킨 타입 안내 올리브영 성조 스킨 촉촉 타입 운영 죽다 이다 구매 참고 바...  \n",
       "197  피부 전문가 의하다 만들다 어진 피부 전문 코스 브랜드 말로 서로 피부 밉다 해결 ...  \n",
       "198  상품 설명 프록실린 성분 삼나무 추출 성분 즉각 피부 진정 피부 탄력 증가 토너 사...  \n",
       "\n",
       "[199 rows x 5 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advertisement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>단독 어워즈 한정 기획 버워스 토너 부문 단독 어워즈 한정 기획 아누 어성초 수딩 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>올리브영 어워즈 진정 케어 기준 아비 어성초 토너 스킨 토너 어성초 토너 더블 어워...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>크림 스킨 리필팩 기획 세트 기획 세트 기획 세트 구성 크림 스킨 리필팩 오특 비아...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>단독 구성 자작나무 수분 크림 수분 진정 비타 일루 동시 차오 수분 필요 지치다 피...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>강력 트리 싸르 케어 올리브영 대표 트러블 토너 수상 올리브영 어워즈 스킨 토너 부...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>피부 진정 순도 아줄렌 놀랍다 진정 능력 라인 민감 피부 찾다 저자극 진정 토너 더...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>단독 기획 어성초 토너 대용량 기획 진정 토너 저자극 진정 진정 저자극 피부결 케어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>어성초 스킨 타입 안내 올리브영 성조 스킨 촉촉 타입 운영 죽다 이다 구매 참고 바...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>피부 전문가 의하다 만들다 어진 피부 전문 코스 브랜드 말로 서로 피부 밉다 해결 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>상품 설명 프록실린 성분 삼나무 추출 성분 즉각 피부 진정 피부 탄력 증가 토너 사...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    ad\n",
       "0    단독 어워즈 한정 기획 버워스 토너 부문 단독 어워즈 한정 기획 아누 어성초 수딩 ...\n",
       "1    올리브영 어워즈 진정 케어 기준 아비 어성초 토너 스킨 토너 어성초 토너 더블 어워...\n",
       "2    크림 스킨 리필팩 기획 세트 기획 세트 기획 세트 구성 크림 스킨 리필팩 오특 비아...\n",
       "3    단독 구성 자작나무 수분 크림 수분 진정 비타 일루 동시 차오 수분 필요 지치다 피...\n",
       "4    강력 트리 싸르 케어 올리브영 대표 트러블 토너 수상 올리브영 어워즈 스킨 토너 부...\n",
       "..                                                 ...\n",
       "194  피부 진정 순도 아줄렌 놀랍다 진정 능력 라인 민감 피부 찾다 저자극 진정 토너 더...\n",
       "195  단독 기획 어성초 토너 대용량 기획 진정 토너 저자극 진정 진정 저자극 피부결 케어...\n",
       "196  어성초 스킨 타입 안내 올리브영 성조 스킨 촉촉 타입 운영 죽다 이다 구매 참고 바...\n",
       "197  피부 전문가 의하다 만들다 어진 피부 전문 코스 브랜드 말로 서로 피부 밉다 해결 ...\n",
       "198  상품 설명 프록실린 성분 삼나무 추출 성분 즉각 피부 진정 피부 탄력 증가 토너 사...\n",
       "\n",
       "[199 rows x 1 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 광고 문구 안에서만 볼거니까 ad만 남김.\n",
    "advertisement_ad = advertisement.iloc[:, 4:]\n",
    "advertisement_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(\n",
    "                        max_df=0.6,\n",
    "                        min_df=1,\n",
    "                        max_features=100) # 자주 등장하는 단어 5000개로 제한\n",
    "X = count.fit_transform(advertisement_ad['ad'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 48)\t5\n",
      "  (0, 45)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 49)\t5\n",
      "  (0, 22)\t2\n",
      "  (0, 31)\t2\n",
      "  (0, 33)\t1\n",
      "  (0, 84)\t2\n",
      "  (0, 89)\t3\n",
      "  (0, 51)\t2\n",
      "  (0, 68)\t1\n",
      "  (0, 67)\t4\n",
      "  (0, 46)\t5\n",
      "  (0, 9)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 62)\t3\n",
      "  (0, 27)\t1\n",
      "  (0, 78)\t5\n",
      "  (0, 96)\t2\n",
      "  (0, 29)\t1\n",
      "  (0, 75)\t2\n",
      "  (0, 14)\t1\n",
      "  (0, 86)\t2\n",
      "  (0, 91)\t8\n",
      "  :\t:\n",
      "  (196, 15)\t1\n",
      "  (196, 41)\t1\n",
      "  (197, 45)\t1\n",
      "  (197, 33)\t2\n",
      "  (197, 96)\t2\n",
      "  (197, 21)\t1\n",
      "  (197, 2)\t1\n",
      "  (197, 59)\t1\n",
      "  (197, 7)\t1\n",
      "  (197, 43)\t1\n",
      "  (197, 98)\t1\n",
      "  (197, 77)\t1\n",
      "  (197, 23)\t2\n",
      "  (197, 58)\t1\n",
      "  (197, 92)\t1\n",
      "  (197, 56)\t4\n",
      "  (197, 17)\t1\n",
      "  (197, 24)\t2\n",
      "  (197, 39)\t1\n",
      "  (197, 8)\t1\n",
      "  (198, 78)\t1\n",
      "  (198, 94)\t1\n",
      "  (198, 83)\t1\n",
      "  (198, 30)\t2\n",
      "  (198, 26)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T22:41:05.077075700Z",
     "start_time": "2023-12-12T22:41:05.061041300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LatentDirichletAllocation\n\u001B[0;32m      3\u001B[0m lda \u001B[38;5;241m=\u001B[39m LatentDirichletAllocation(n_components \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, \n\u001B[0;32m      4\u001B[0m                                 max_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m,\n\u001B[0;32m      5\u001B[0m                                 random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m,\n\u001B[0;32m      6\u001B[0m                                 verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m      7\u001B[0m                                learning_offset \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m100\u001B[39m) \u001B[38;5;66;03m# verbose 매개변수 사용\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m X_topics \u001B[38;5;241m=\u001B[39m lda\u001B[38;5;241m.\u001B[39mfit_transform(X)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components = 3, \n",
    "                                max_iter = 50,\n",
    "                                random_state=1000,\n",
    "                                verbose=True,\n",
    "                               learning_offset = 100) # verbose 매개변수 사용\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "시험 트리 추출 스킨 개선 테스트 완료 여드름 피지 적용 인체 효과 기간 크림 탄력 결과 성인 직후 감소 트러블 적합 여성 오일 과학 어요 강력 원료 대상 고민 하이 씨드 글라이콜 단계 기준 도움 특성 효능 장벽 뷰티 정제 가능 이드 크다 제형 위해 인증 바르다 주다 보다 부위 해주 되어다 기능 이다 타입 저자극 촉촉하다 좋다 워터 화장솜 공급 들다 어성초 강화 민감 빠르다 화장품 각질 받다 보호 필요 데일리 채우다 용기 순하다 건강 함유 보습 가다 모공 건조 적다 따르다 보관 모든 적당 피부결 히알루론산 늘다 흡수 덜다 부드럽다 제조 에센스 세안 정돈 세라 면도 브랜드 클럽\n",
      "Topic 2:\n",
      "각질 이다 모공 면도 어성초 피지 클럽 화장품 피부결 받다 크다 부드럽다 세안 정돈 용기 되어다 브랜드 보관 추출 화장솜 제조 따르다 필요 보다 기능 적다 공급 흡수 들다 늘다 적당 덜다 도움 바르다 저자극 부위 좋다 해주 가능 건강 타입 순하다 모든 개선 주다 위해 트러블 적합 민감 보습 특성 정제 에센스 빠르다 고민 데일리 가다 크림 함유 보호 테스트 기준 인체 감소 건조 하이 효과 촉촉하다 스킨 결과 적용 채우다 워터 인증 단계 어요 글라이콜 원료 뷰티 오일 과학 완료 성인 시험 직후 여드름 제형 기간 여성 대상 히알루론산 이드 효능 강력 탄력 씨드 장벽 강화 트리 세라\n",
      "Topic 3:\n",
      "보습 스킨 추출 민감 효과 워터 흡수 장벽 촉촉하다 히알루론산 이드 건조 피부결 보다 함유 부드럽다 하이 좋다 각질 오일 크림 빠르다 건강 되어다 채우다 받다 가다 씨드 세안 원료 단계 어요 바르다 세라 강화 기준 제형 인증 이다 화장품 보호 개선 따르다 데일리 글라이콜 화장솜 필요 저자극 테스트 에센스 타입 완료 공급 뷰티 모든 덜다 해주 가능 주다 부위 정돈 순하다 효능 적당 보관 적다 위해 대상 고민 과학 정제 도움 특성 들다 강력 늘다 기능 여성 제조 크다 용기 브랜드 적용 결과 직후 기간 탄력 성인 트러블 인체 적합 모공 감소 시험 피지 트리 여드름 면도 어성초 클럽\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 100 # 토픽 관련 단어 20개 보기\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx + 1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###진정: 티트리 여드름 피지 감소 트러블 저자극 어성초 민감 순하다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###피부결: 각질 모공 피부결 크다 부드럽다 정돈 화장솜 장벽 탄력 보호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###보습: 보습 워터 흡수 촉촉하다 히알루론산 건조 오일 채우다 강화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T23:10:47.500730500Z",
     "start_time": "2023-12-12T23:10:47.472122600Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'token2id'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyLDAvis\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgensim_models\u001B[39;00m\n\u001B[0;32m      4\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39menable_notebook()\n\u001B[1;32m----> 5\u001B[0m vis \u001B[38;5;241m=\u001B[39m pyLDAvis\u001B[38;5;241m.\u001B[39mgensim_models\u001B[38;5;241m.\u001B[39mprepare(lda, X, count)\n\u001B[0;32m      6\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39mdisplay(vis)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:122\u001B[0m, in \u001B[0;36mprepare\u001B[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprepare\u001B[39m(topic_model, corpus, dictionary, doc_topic_dist\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     78\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03m    the data structures needed for the visualization.\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001B[39;00m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     opts \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pyLDAvis\u001B[38;5;241m.\u001B[39mprepare(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mopts)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:23\u001B[0m, in \u001B[0;36m_extract_data\u001B[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001B[0m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m# Need corpus to be a streaming gensim list corpus for len and inference functions below:\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     corpus \u001B[38;5;241m=\u001B[39m gensim\u001B[38;5;241m.\u001B[39mmatutils\u001B[38;5;241m.\u001B[39mSparse2Corpus(corpus_csc)\n\u001B[1;32m---> 23\u001B[0m vocab \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(dictionary\u001B[38;5;241m.\u001B[39mtoken2id\u001B[38;5;241m.\u001B[39mkeys())\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# for now, I'll just make sure we don't ever get zeros...\u001B[39;00m\n\u001B[0;32m     26\u001B[0m beta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.01\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CountVectorizer' object has no attribute 'token2id'"
     ]
    }
   ],
   "source": [
    "# 시각화 - 버전 2 genim_models 난 원래 이게 됐었던 것 같은데 안되네..?\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, X, count)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-12T23:09:58.301857100Z",
     "start_time": "2023-12-12T23:09:55.910447900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 50\n",
      "iteration: 2 of max_iter: 50\n",
      "iteration: 3 of max_iter: 50\n",
      "iteration: 4 of max_iter: 50\n",
      "iteration: 5 of max_iter: 50\n",
      "iteration: 6 of max_iter: 50\n",
      "iteration: 7 of max_iter: 50\n",
      "iteration: 8 of max_iter: 50\n",
      "iteration: 9 of max_iter: 50\n",
      "iteration: 10 of max_iter: 50\n",
      "iteration: 11 of max_iter: 50\n",
      "iteration: 12 of max_iter: 50\n",
      "iteration: 13 of max_iter: 50\n",
      "iteration: 14 of max_iter: 50\n",
      "iteration: 15 of max_iter: 50\n",
      "iteration: 16 of max_iter: 50\n",
      "iteration: 17 of max_iter: 50\n",
      "iteration: 18 of max_iter: 50\n",
      "iteration: 19 of max_iter: 50\n",
      "iteration: 20 of max_iter: 50\n",
      "iteration: 21 of max_iter: 50\n",
      "iteration: 22 of max_iter: 50\n",
      "iteration: 23 of max_iter: 50\n",
      "iteration: 24 of max_iter: 50\n",
      "iteration: 25 of max_iter: 50\n",
      "iteration: 26 of max_iter: 50\n",
      "iteration: 27 of max_iter: 50\n",
      "iteration: 28 of max_iter: 50\n",
      "iteration: 29 of max_iter: 50\n",
      "iteration: 30 of max_iter: 50\n",
      "iteration: 31 of max_iter: 50\n",
      "iteration: 32 of max_iter: 50\n",
      "iteration: 33 of max_iter: 50\n",
      "iteration: 34 of max_iter: 50\n",
      "iteration: 35 of max_iter: 50\n",
      "iteration: 36 of max_iter: 50\n",
      "iteration: 37 of max_iter: 50\n",
      "iteration: 38 of max_iter: 50\n",
      "iteration: 39 of max_iter: 50\n",
      "iteration: 40 of max_iter: 50\n",
      "iteration: 41 of max_iter: 50\n",
      "iteration: 42 of max_iter: 50\n",
      "iteration: 43 of max_iter: 50\n",
      "iteration: 44 of max_iter: 50\n",
      "iteration: 45 of max_iter: 50\n",
      "iteration: 46 of max_iter: 50\n",
      "iteration: 47 of max_iter: 50\n",
      "iteration: 48 of max_iter: 50\n",
      "iteration: 49 of max_iter: 50\n",
      "iteration: 50 of max_iter: 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 16\u001B[0m\n\u001B[0;32m     13\u001B[0m X_topics \u001B[38;5;241m=\u001B[39m lda\u001B[38;5;241m.\u001B[39mfit_transform(X)\n\u001B[0;32m     15\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39menable_notebook()\n\u001B[1;32m---> 16\u001B[0m panel \u001B[38;5;241m=\u001B[39m pyLDAvis\u001B[38;5;241m.\u001B[39msklearn\u001B[38;5;241m.\u001B[39mprepare(lda, X, count)  \u001B[38;5;66;03m# Use 'pyLDAvis.sklearn.prepare'\u001B[39;00m\n\u001B[0;32m     17\u001B[0m pyLDAvis\u001B[38;5;241m.\u001B[39mdisplay(panel)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyLDAvis\\sklearn.py:95\u001B[0m, in \u001B[0;36mprepare\u001B[1;34m(lda_model, dtm, vectorizer, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Create Prepared Data from sklearn's LatentDirichletAllocation and CountVectorizer.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     92\u001B[0m \u001B[38;5;124;03mSee `pyLDAvis.prepare` for **kwargs.\u001B[39;00m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     94\u001B[0m opts \u001B[38;5;241m=\u001B[39m fp\u001B[38;5;241m.\u001B[39mmerge(_extract_data(lda_model, dtm, vectorizer), kwargs)\n\u001B[1;32m---> 95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pyLDAvis\u001B[38;5;241m.\u001B[39mprepare(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mopts)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyLDAvis\\_prepare.py:398\u001B[0m, in \u001B[0;36mprepare\u001B[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001B[0m\n\u001B[0;32m    391\u001B[0m         mds \u001B[38;5;241m=\u001B[39m js_PCoA\n\u001B[0;32m    393\u001B[0m \u001B[38;5;66;03m# Conceptually, the items in `topic_term_dists` end up as individual rows in the\u001B[39;00m\n\u001B[0;32m    394\u001B[0m \u001B[38;5;66;03m# DataFrame, but we can speed up ingestion by treating them as columns and\u001B[39;00m\n\u001B[0;32m    395\u001B[0m \u001B[38;5;66;03m# transposing at the end. (This is especially true when the number of terms far\u001B[39;00m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;66;03m# exceeds the number of topics.)\u001B[39;00m\n\u001B[0;32m    397\u001B[0m topic_term_dist_cols \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 398\u001B[0m     pd\u001B[38;5;241m.\u001B[39mSeries(topic_term_dist, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat64\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    399\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m topic_term_dist \u001B[38;5;129;01min\u001B[39;00m topic_term_dists\n\u001B[0;32m    400\u001B[0m ]\n\u001B[0;32m    401\u001B[0m topic_term_dists \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(topic_term_dist_cols, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mT\n\u001B[0;32m    403\u001B[0m topic_term_dists \u001B[38;5;241m=\u001B[39m _df_with_names(topic_term_dists, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtopic\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mterm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\pyLDAvis\\_prepare.py:232\u001B[0m, in \u001B[0;36m_topic_info\u001B[1;34m(topic_term_dists, topic_proportion, term_frequency, term_topic_freq, vocab, lambda_step, R, n_jobs)\u001B[0m\n\u001B[0;32m    230\u001B[0m tt_sum \u001B[38;5;241m=\u001B[39m topic_term_dists\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m    231\u001B[0m topic_given_term \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39meval(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtopic_term_dists / tt_sum\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 232\u001B[0m log_1 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(pd\u001B[38;5;241m.\u001B[39meval(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtopic_given_term.T / topic_proportion\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m    233\u001B[0m kernel \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39meval(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtopic_given_term * log_1.T\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    234\u001B[0m distinctiveness \u001B[38;5;241m=\u001B[39m kernel\u001B[38;5;241m.\u001B[39msum()\n",
      "\u001B[1;31mTypeError\u001B[0m: DataFrame.drop() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# 시각화 버전 1  - sklearn \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "advertisement = pd.read_csv('../docs/oliveyoung_advertisement_preprocessed.csv')\n",
    "advertisement_ad = advertisement.iloc[:, 4:]\n",
    "\n",
    "count = CountVectorizer(max_df=0.6, min_df=1, max_features=100)\n",
    "X = count.fit_transform(advertisement_ad['ad'].values)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=3, max_iter=50, random_state=1000, verbose=True)\n",
    "X_topics = lda.fit_transform(X)\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda, X, count)  # Use 'pyLDAvis.sklearn.prepare'\n",
    "pyLDAvis.display(panel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
